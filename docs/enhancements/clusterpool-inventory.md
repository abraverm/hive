# Clusterpool for on-prem cloud providers

[HIVE-1367](https://issues.redhat.com/browse/HIVE-1367)

- [Clusterpool for on-prem cloud providers](#clusterpool-for-on-prem-cloud-providers)
  - [Summary](#summary)
  - [Problem Statement](#problem-statement)
  - [Proposal](#proposal)
    - [Summary](#summary-1)
    - [`ClusterPool.Spec.Inventory`](#clusterpoolspecinventory)
    - [`ClusterPool.Status.Inventory`](#clusterpoolstatusinventory)
    - [How To Use](#how-to-use)
    - [Validation](#validation)
    - [Pool Version](#pool-version)
    - [Handling Inventory Updates](#handling-inventory-updates)
      - [Adding An Inventory](#adding-an-inventory)
      - [Adding An Entry to the Inventory](#adding-an-entry-to-the-inventory)
      - [Removing An Entry from the Inventory](#removing-an-entry-from-the-inventory)
      - [Deleting The Inventory](#deleting-the-inventory)
    - [Maintaining the lease of the ClusterDeploymentCustomization](#maintaining-the-lease-of-the-clusterdeploymentcustomization)
    - [Fairness](#fairness)
  - [Future](#future)
  - [Alternatives](#alternatives)
    - [Bespoke Inventory Definition](#bespoke-inventory-definition)
    - [Full Spec](#full-spec)
    - [Hooks](#hooks)
        
## Summary

As a cluster administrator, I want to be able to use ClusterPools to provision clusters on on-prem clouds like vSphere, OpenStack and Baremetals.

## Problem Statement
Provisioning a cluster requires configuration unique to that cluster.
For example, the cluster's name is used to build the hostnames for the API and console.
_Someone_ has to handle the DNS that resolves those hostnames to the IPs of the services created by the installer.
One option is to create your ClusterDeployment with `ManageDNS=True` and we'll create a DNSZone on the fly.

But that doesn't work for some cloud providers such a vSphere, OpenStack and Baremetals.

In such cases, one solution is to configure DNS manually and then create your ClusterDeployment with the right `Name` and `BaseDomain` so that the assembled hostnames match those entries. This is fine if you're creating ClusterDeployments by hand, but breaks down if ClusterDeployment names are being generated with random slugs, as is the case with ClusterPools.

## Proposal
### Summary
Allow ClusterPool to accept an inventory of ClusterDeploymentCustomizations that will have install config json patches to be used when generating ClusterDeployments for the pool.

### `ClusterPool.Spec.Inventory`
Add a field to `ClusterPool.Spec` called `Inventory`.

It has an array of object references to ClusterDeploymentCustomization custom resource. This CR will have a json patch (RFC 6902) for default install config generated by clusterpool controller. The json patch will have cluster specific changes, for vSphere this corresponds to configured DNS hostnames

```yaml
spec:
  inventory:
    ClusterDeploymentCustomizations:
      - name: foo-cluster-deployment-customization
      - name: bar-cluster-deployment-customization
      - name: baz-cluster-deployment-customization
```

and ClusterDeploymentCustomization CR will look like
```yaml
apiVersion: v1
kind: ClusterDeploymentCustomization
metadata:
  name: foo-cluster-deployment-customization
  namespace: my-project
spec:
  installConfigPatches:
    - op: replace
      path: metadata/name
      value: foo
status:
  clusterDeploymentRef: 
    name: foo
    namespace: foo-namespace
  conditions:
    - lastProbeTime: "2020-11-05T14:49:26Z"
      lastTransitionTime: "2020-11-05T14:49:26Z"
      message: Currently in use by cluster deployment foo of clusterpool foo-pool
      reason: ClusterDeploymentCustomizationInUse
      status: False
      type: Available
```

if DNS is configured with the name `foo`, ClusterDeploymentCustomization.spec.installConfigPatches content to patch vSphere install config will be as follows
```yaml
spec:
  installConfigPatches:
    - op: replace
      path: metadata/name
      value: foo
```

When adding a ClusterDeployment, if such a `Inventory` is present, ClusterPool controller will:
1. Pick one of the inventory items that are available.  Availability is determined by their status in `status.inventory` and verified with the actual ClusterDeploymentCustomization resource. 
2. Chosen inventory resource will be reserved by updating the inventory status and the resource condition to `Reserved`.
3. Apply the patches in `spec.installConfigPatches` on the install config that was generated by ClusterPool. The pre customized install config would be the same as if `Inventory` wasn't set. The patches will be applied in the order listed in the inventory. On failure, ClusterPool update the inventory item status as `Broken by configuration`, the resource condition will change to `Available` but no longer be used by ClusterPool until updated.
4. On creation of ClusterDeployment, the inventory item will be referenced in CD `spec.ClusterPoolReference.ClusterDeploymentCustomizationRef` and `inventory.status` field `ClusterDeploymentRef` with reference to the new CD.
5. ClusterDeployment will have additional Finalizer that prevents it from getting deleted before releasing the inventory resource - updating the resource condition to `Available`. Failed provisioning will cause the ClusterPool to release the resource, and reducing its status `Attempts` field by 1. When inventory item `Attempts` reaches 0, its status changes to `Broken by cloud` - meaning ClusterPool is unable to provision cluster on the specific cloud with given customization. The status and attempts will reset on resource update.

Absent a `Inventory`, ClusterPool will continue to use the generated default install config as it does today.

### `ClusterPool.Status.Inventory`
Add a field to `ClusterPool.Status` called `Inventory`.

Inventory resources might be used by multiple ClusterPools and the results can vary. For example the resource might be broken due to mismatch ClusterPool specification or with the cloud environment. Inventory resource status is tracked in ClusterPool's `Status.Inventory.<resource name>` property, which contains the following:
- `Version` - Inventory resource last seen version.
- `Status` - The current status of the resource from ClusterPool's perspective:
  - `Available` - Inventory resource is available.
  - `Reserved` - This ClusterPool is using the inventory resource in one of its ClusterDeployments.
  - `Broken by configuration` - ClusterPool was unable to apply inventory resource due to configuration mismatch, for example a bad patch.
  - `Broken by cloud` - All ClusterDeployments failed to provision after number of `Attempts`.
  - `Missing` -  Inventory resource doesn't exist but listed on ClusterPool inventory.
  - `Unavailable` - Inventory resource used by other ClusterPool.
  - `To Be Deleted` - Inventory resource is removed from ClusterPool `Spec.Inventory` and will be deleted from `Status.Inventory` once CD is deleted.
  - `To Be Updated` - Inventory is currently reserved by a ClusterDeployment and needs to be updated once released.
- `Attempts` -  Number of attempts left before marked as `Broken by cloud`.
- `ClusterDeployment` - Reference to ClusterPool's ClusterDeployment using the inventory resource.

Periodically and on usage ClusterPool will update the `Status.Inventory`, for example:
- if inventory has a reference to a ClusterDeploymentCustomization that does not exist, we log an error, along with status `Missing` in ClusterPool `Status.Inventory`, and move on to the next entry in the list. 
- if inventory has a reference to malformed ClusterDeploymentCustomization, we log an error, along with a status `Broken by configuration` and move on to the next entry in the list.

ClusterPool's inventory condition summarizes the `Status.Inventory` and the general state:
 - ClusterPool size vs inventory resource size and status

### How To Use
For the VSphere case, this allows the administrator to:
- Preconfigure DNS with following entries (assuming cluster name is `foo`)
    ```
    10.0.0.10  api.foo.example.com
    10.0.0.11  apps.foo.example.com
    ```
- Create a ClusterDeploymentCustomization CR to patch `spec.metadata.name` field of the default install config generated by clusterpool controller. Please refer the section above of a sample CR. The content in `spec.installConfigPatches` field should be as follows
  ```yaml
  spec:
  installConfigPatches:
  - op: replace
    path: metadata/name
    value: foo
  ```
- Add the name of ClusterDeploymentCustomization CR to `clusterPool.spec.inventory.ClusterDeploymentCustomizations` list. For ClusterDeploymentCustomization with a name `foo-cluster-deployment-customization` the clusterpool should be configured as follows
    ```yaml
    spec:
      inventory:
        ClusterDeploymentCustomizations:
          - name: foo-cluster-deployment-customization
    ```

### Validation
Webhook validation will ensure that for a clusterpool
- if `spec.inventory.ClusterDeploymentCustomizations` is specified, it is not an empty list.

### Pool Version
To make [adding](#adding-an-inventory) and [deleting](#deleting-the-inventory) `Inventory` work sanely, we will adjust the computation of the pool version used for [stale CD detection/replacement](https://issues.redhat.com/browse/HIVE-1058) as follows:
- When `Inventory` is present, append an arbitrary fixed string before the [final hash operation](https://github.com/openshift/hive/blob/0b9229b91e6f8a3c2bf095efbdf017226e69d026/pkg/controller/clusterpool/clusterpool_controller.go#L479).
  (We don't want to recalculate the hash any time the inventory changes, as this would treat *all* existing unclaimed CDs as stale and replace them.)
- When `Inventory` is absent, compute the pool version as before.
  This ensures the version does not change (triggering replacement of all unclaimed CDs) for existing pools when hive is upgraded to include this feature.

### Handling Inventory Updates
#### Adding An Inventory
Adding an `Inventory` to a ClusterPool which previously didn't have one will cause the controller to [recompute the pool version](#pool-version), rendering all existing unclaimed clusters stale, causing them to be replaced gradually.

#### Adding An Entry to the Inventory
The `Status.Inventory` is updated with a new entry, if it doesn't exist.

#### Removing An Entry from the Inventory
- If the entry is not `Reserved` and there is no reference in `ClusterDeployment`, then it will be removed from `Status.Inventory`
- If an _unclaimed_ CD exists with that name, we update the entry status to `To Be Deleted`, delete the CD and then remove it from `Status.Inventory`.
- If a _claimed_ CD exists with that name, we update the entry status to `To Be Deleted`,, and remove it from `Status.Inventory` once the CD is deleted.

#### Deleting The Inventory
This will change the [pool version](#pool-version), update the status of all entries to `To Be Deleted`, rendering existing unclaimed clusters stale and causing the controller to replace them gradually.
The administrator may wish to speed up this process by manually deleting CDs, or scaling the pool size to zero and back.

#### Updating Inventory Resource
- If the entry is not `Reserved` and there is no `ClusterDeployment` reference, then `Attempts` is reset, `Version` is set to the new version of the resource and its new status is `Available`
- If the entry is `Reserved`, then it is marked as `To Be Updated` and if ClusterDeployment is _unclaimed_ then it is marked stale.

### Maintaining the lease of the ClusterDeploymentCustomization
If two controller pods trying to build a ClusterDeployment for the ClusterPool end up fetching the same ClusterDeploymentCustomization, they will be trying to claim same configured DNS or a hostname - a chaos scenario. To avoid it, we need to maintain a lease for each ClusterDeploymentCustomization. There are five points of reference for maintaining a lease:
- ClusterPool `status.inventory` tracks status of each ClusterDeploymentCustomization resource it has listed in `spec.Inventory`. Each item contains `Status` and `ClusterDeployment` field. If ClusterDeploymentCustomization is reserved by this ClusterPool, the `Status` will be `Reserved` and `ClusterDeployment` will have reference to ClusterDeployment it is being used at.
- Customized ClusterDeployment will have reference to the ClusterDeploymentCustomization in `spec.clusterPoolReference.ClusterDeploymentCustomizationRef`.
- ClusterDeploymentCustomization has `Available` condition to track if it is used by any ClusterPool, and reference to the active ClusterDeployment in `status.ClusterDeploymentRef`

Each ClusterPool updates the status of its own inventory and validates the status of ClusterDeploymentCustomization. When there is mismatch between the ClusterPool inventory status and ClusterDeploymentCustomization, ClusterPool will try to resolve the problem in favor of ClusterDeploymentCustomization. For example, if ClusterDeploymentCustomization is Available and doesn't have reference to a ClusterDeployment, then ClusterPool will update it status to reserved and reference to the relevant ClusterDeployment.

### Fairness
We will assume it is **not** important that we rotate through the list of supplied install config patches in any particular order, or with any nod to "fairness".
For example: If there are five customizations and the usage pattern happens to only use two at a time, there is no guarantee that we will round-robin through all five.
We may reuse the same two over and over, or pick at random, or something else -- any of which has the potential to "starve" some patches.

## Future
Recognizing that there are other aspects of the ClusterDeployment that may need to be customizable for inventory purposes, we've left this design open to adding e.g. `clusterDeploymentSpecPatches` to the ClusterDeploymentCustomization spec.

## Alternatives
In conceiving this design, a number of alternative designs were considered:

### Bespoke Inventory Definition
As in a [precursor to this RFE](https://github.com/openshift/hive/pull/1549), creating a bespoke inventory structure with hand-picked fields whose behavior has to be custom implemented every time, and may even need different branches for different cloud providers, depending where the values actually have to land.
E.g. install configs have different structures for different cloud providers.

**Pro:** Simple UX.

**Con:** Difficult to code and maintain, since we have to do new work every time we need to support a new field or provider, or even in cases where underlying formats/APIs change.

### Full Spec
The inventory is simply (a ref to) a list of opaque install configs, or even ClusterDeploymentSpecs.

**Pro:** One and done coding for the hive team (modulo bugs). We never again have to introspect install configs.

**Con:** This is pretty terrible UX. The user has to maintain an entire install config for every possible cluster.
If all you need is e.g. a different name/IP for each, that's really heavy.
The user probably needs to go write their own templating tool, etc.
Also, this is potentially rife for abuse: nothing would be stopping the user from making their pool truly heterogeneous, potentially even to the point of different cloud providers.
This is anathema to the philosophy of ClusterPool, where every cluster is supposed to be substantially "the same".

### Hooks
Support a hook calling some external service to mutate install config and/or ClusterDeployment spec.

**Pro:** Opacity for hive again. Ultimate flexibility for the user again.

**Con:** *Really* heavy for the user – now they have to write code and deploy a service.
